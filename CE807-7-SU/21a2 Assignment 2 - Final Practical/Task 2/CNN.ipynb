{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKF6889Q7Odk",
        "outputId": "01264a97-b6bb-4996-af77-fe127f28612e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1264M  100 1264M    0     0   180M      0  0:00:07  0:00:07 --:--:--  176M\n"
          ]
        }
      ],
      "source": [
        "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.79 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header \"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/19426/25246/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220802%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220802T045418Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=799a354a79438738ee029cd982685432b434d098a304f558d0ffef95a8d446169d3adc6570276c350742527a34a46dc20fbbe3e788353115fec117c92887930632f214a0fd4210f59fd402a47f00b2e449d39b7114f62b7dfdf1e60b6dfc318623866c25b6a6a168b62de2770f031a238ab2c447a74d640470bf6f86b6962eaa8c1d8a09479c5730c7b83e0b359113954a30fcb36d433db6cbf3f9307f2642cd0343418c89c3fe2369860eb79d196f50afff0fed81328c7e1ab182248b9b6f48f362de497dedd2d15c4a6f1dab6d147a1bf207baba371bb4d41e12dc8de30989803fb2ba9524406772c3b1c4e8cf4335c7825ae448293e0093197dc90ea9e6df\" -L -o \"archive.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtQGBcQo7dHv",
        "outputId": "d01a2e7c-a331-4e84-e00a-f63342dacdfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "replace econbiz.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: econbiz.csv             \n",
            "replace pubmed.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: pubmed.csv              y\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization,Embedding\n",
        "from keras.models import Sequential\n",
        "#from keras.optimizers import Adam\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gacd_1C67dwI",
        "outputId": "c2a37511-6e2e-4f16-e656-55770d8d5c1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load econbiz dataset (use only a portion to see pre-processing part)\n",
        "data = pd.read_csv(\"econbiz.csv\")\n",
        "data = data.iloc[:100000]\n",
        "data = data[['id','title','labels']]"
      ],
      "metadata": {
        "id": "D34HE6io7eYC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate labels (currently separeted by tab space) and this is needed for the multilabelbinarizer\n",
        "data['labels'] = data['labels'].str.split()"
      ],
      "metadata": {
        "id": "M4tV18N17fCl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-process title data\n",
        "def process(title):\n",
        "\n",
        "    stop_words = set(stopwords.words('english')) #Stop words to remove\n",
        "    new_df = re.sub('[^a-zA-Z]', ' ', title) #Removes numbers\n",
        "    new_df = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', new_df) # Removes single characters\n",
        "    new_df = re.sub(r'\\s+', ' ', new_df) # Removes multiple spaces\n",
        "    tokens = word_tokenize(new_df) #Tokenize the title sample for checking stopwords\n",
        "    new_df = [token for token in tokens if not token in stop_words]\n",
        "\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "K8YVto9a7e_q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess title column\n",
        "X = []\n",
        "new_df = list(data['title'])\n",
        "\n",
        "for idx,title in enumerate(new_df):\n",
        "  X.append(process(title))\n",
        "\n",
        "#Preprocess labels to be a binary representation\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(data.labels)\n",
        "labels = multilabel_binarizer.classes_\n",
        "y = multilabel_binarizer.transform(data.labels)"
      ],
      "metadata": {
        "id": "GlO4aXV17e84"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide dataset into training and validation set (80/20)\n",
        "#To this point, the title column still needs to be vectorize\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)"
      ],
      "metadata": {
        "id": "BGW4Mh9G7e5i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize tokenizer from keras that will vectorize title values\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "aKGucSY-7e12"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "x_train = pad_sequences(x_train, padding= 'post',maxlen = 51)\n",
        "x_test = pad_sequences(x_test,padding='post', maxlen = 51)\n",
        "print('Pad sequences (samples x time)')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0FHKinG7etp",
        "outputId": "bf0fbc9f-d06f-4335-df29-6f8d96a4d7c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (80000, 51)\n",
            "x_test shape: (20000, 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Got the model structure and parameters from https://keras.io/examples/imdb_cnn/ which is an example of using CNN for text classification\n",
        "# set parameters:\n",
        "max_features = vocabulary_size\n",
        "maxlen = 51\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 300\n",
        "epochs = 5\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(y_train.shape[1]))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0jJD-KT9fi3",
        "outputId": "2559368a-659a-489a-aa98-95e6869fe51b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 144s 57ms/step - loss: 0.0102 - accuracy: 0.0034 - val_loss: 0.0084 - val_accuracy: 0.0049\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 144s 58ms/step - loss: 0.0059 - accuracy: 0.0065 - val_loss: 0.0092 - val_accuracy: 0.0142\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 152s 61ms/step - loss: 0.0057 - accuracy: 0.0149 - val_loss: 0.0093 - val_accuracy: 0.0314\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 151s 60ms/step - loss: 0.0053 - accuracy: 0.0299 - val_loss: 0.0065 - val_accuracy: 0.0492\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 146s 58ms/step - loss: 0.0050 - accuracy: 0.0469 - val_loss: 0.0053 - val_accuracy: 0.0683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20fdf8d810>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.arange(0.05,1.0,0.05)\n",
        "scores = []\n",
        "\n",
        "for prob in probs:\n",
        "  preds = model.predict(x_test)\n",
        "  preds[preds>=prob] = 1\n",
        "  preds[preds<prob] = 0\n",
        "  scores.append(tuple((f1_score(y_test, preds, average=\"samples\"),prob)))\n",
        "  print((f1_score(y_test, preds, average=\"samples\"),prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9Ebyrtg9j8X",
        "outputId": "80751e2e-27fc-4924-95c0-acc89fc7a014"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.11778730099416852, 0.05)\n",
            "(0.1825840305749914, 0.1)\n",
            "(0.2061008597658147, 0.15000000000000002)\n",
            "(0.2021330298811208, 0.2)\n",
            "(0.18104751101366814, 0.25)\n",
            "(0.15295772772903948, 0.3)\n",
            "(0.12615350527525448, 0.35000000000000003)\n",
            "(0.1007298626210391, 0.4)\n",
            "(0.07821868185735834, 0.45)\n",
            "(0.057974582623259094, 0.5)\n",
            "(0.041670245016421484, 0.55)\n",
            "(0.029764802403478872, 0.6000000000000001)\n",
            "(0.022339221872898343, 0.6500000000000001)\n",
            "(0.01606924963924964, 0.7000000000000001)\n",
            "(0.011276699134199134, 0.7500000000000001)\n"
          ]
        }
      ]
    }
  ]
}