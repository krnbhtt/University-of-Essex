%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Registration Number
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}
\usepackage{acm, amssymb,amsmath,epsfig,lipsum}
\usepackage[colorlinks=true,bookmarks=false,citecolor=blue,urlcolor=blue,hidelinks]{hyperref}

\setcounter{page}{1}
\sloppy     %line breaks
\ninept

\title{TASK 3: XMLC Report }
\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother \name{{\em Registration Number: 2112102}}
\address{\\July 2022 {\small \tt}}


\begin{document}
\maketitle
\section{Motivation}
As a result of the move from analogue to digital representations, massive volumes of data are produced on a regularly. Numerous millions of individuals utilise websites like Google, Ecommerce, and Social Networking sites to post, download, and explore content. It is critical to conduct analysis on it and collect crucial data from a big number of individuals belonging to a variety of distinct categories. As a result, designing a classifier capable of dividing a vast amount of data into its suitable group of categories is a challenging task. The primary goal of the recommendations system is to propose user ideas based upon information that is already available\cite{read2015multi}. The ordinary recommendation engine, on either hand, cannot assess tens of millions objects or labels in real time. Researchers came up with an extreme classification approach to categorise enormous volumes of data in a scenario with numerous categories\cite{li2016weighted}. Using the various categories input into it, the Extreme Multi-Label classifier (XMLC) will build a classification model and predict outcomes about the data were categorized. Various large-scale recommender systems approaches were explored in this study employing the Extreme Multi-Label Classification Method and experimental assessment on two multi-label datasets that can handle vast volumes of data. Each one of these data seemed to be able to deal with enormous volume of data.

\section{Introduction}
The issue of picking the most appropriate portion of classifications to use on each particular article choose among a vast number of potential labels is referred to as extreme multi-label text categorization (XMLC). Because of the fast growth of internet data and the critical need for organisational viewpoints on large sets of data, Wikipedia, for instance, includes over a millions and billions of category labels created by researchers. \cite{bib1}. Those labels have been updated to the website. Every component may meet the requirements for more than just single label.

When we try to compare it with the binary or multi-class classification problems, that have already been extensively analyzed and investigated inside the machine learning articles, multi-label classification represents a profoundly different set of concerns. Because binary classifiers cannot compensate for the relationships which exist among labels, they view class labels as separate target values. \cite{bib2}. This leads to complications whenever it came to classifying several labels. Multi-class classifiers use the unfounded prediction that the class labels remain mutually exclusive to the contexts with multiple labels; more precisely, they presume that content must contain only single class label.

Because of the highly significant imbalanced data situation, dealing with XMLC problems is difficult. Although labels are distributed uniformly across the XMLC databases, it is possible to conclude that a limited amount of training data encounters link to a large number of labels. As a result, understanding the dependence trends and patterns between labels may be challenging \cite{bib3}. When the volume of labels exceeds to thousands or millions in XMLC, the computing expenses of training and testing nonlinear classifiers becomes virtually unaffordable. This is a serious impediment since it renders training and evaluating classifiers complicated.

Significant development has already been achieved in XMLC in past years. \cite{bib4}. A number of solutions have already been developed to solve the complexity of the label space, and even the issues of scalability and data density. In the following part, we will classify XMLC methods into four unique problem-solving approaches.

\begin{itemize}
    \item deep learning approaches.
    \item partitioning methods
    \item embedding-based approaches
    \item one-vs-all approaches.
\end{itemize}

\section{Approaches to XMLC}
\subsection{Deep Learning Approaches}

Deep learning expressions, like TF-IDF features, are expected to recognize meaningful data contained in text data more correctly than bag-of-words features. XML-CNN used CNN approaches is used to simulate text input, whereas AttentionXML and HAXMLNet used attention models to retrieve word embedding of text data. The supervised pre-processed hidden layers produced by using XML-CNN algorithms were used to train the SLICE network. Pre-trained deep language algorithms such as BERT, ELMo, and GPT have recently demonstrated outstanding performance on a variety of Natural language processing tasks. Past research encountered significant training and inference challenges as it wasn't feasible to employ those pre-trained large models for XMC \cite{bib3}. This makes drawing inferences from the data challenging.

\subsection{Embedding Based approaches}
To perform label comparison searching in a reduced subspace, embedding models employ a low-rank representation to explain the label matrices. \cite{bib6}.To represent it different way, embedding-based techniques rely on the assumption that the label space might be expressed by a latent spaces of a lower dimension, and that linked hidden be regarded as correlate to labels of a similar sort. \cite{bib5}. In actuality, embedding-based models outperform minimalistic one-vs-all and partitioning techniques in terms of giving equal gains in compute efficiency. One possibility might be that the label encoding architecture is ineffective.

\subsection{One-Vs-All Approaches}
The one-versus-all technique is overly simplistic, treating every label as if it was one's own distinct binary classification problem. Despite the fact It demonstrated that OVA approaches may achieve a high-accuracy, the training and prediction calculations for those kind of approaches becomes prohibitively expensive whenever the quantity of labels is large. As a result, a number of solutions for improving the algorithm's efficiency have been presented \cite{bib5}. PDSparse accelerates either training and prediction by combining dual and primitive sparsity. We study the use of scalability and sparsity to accelerate the procedure and reduce the total size of the models. OVA approaches are widely applied as the base for a variety of other type of methods.

\subsection{Partitioning}
When splitting up spaces, there seem to be two techniques that might be taken. The procedure of segregating the input data is nothing like the method for dividing a label space. So when result is sparse, just a tiny portion of the labels and illustrations from the label division are available in the input division \cite{bib7}. It is also possible to conduct minimums time prediction depending on label size by employing tree-based algorithms to split the labels. To partition the labels based on the label features collected from the cases, for example, have used a balanced 2-means label tree.

\section{Methods for XMLC}

\subsection{Fast XML}
This represents the present baseline for cutting-edge tree-based XMLC. A structure of training instances is learnt at every cluster of the architecture, and an ND-CG-based objective is modified for optimum performance. The set of articles inside the current node is split up into two subcategories for each cluster, and the initiation of a parametric hyper - plane at every node is evaluated to determine how well the labels within every subgroup must be ordered \cite{bib7}. The basic concept is to guarantee so each subset's articles exhibit identical label distributions, which are then described by the set-specific sorted alphabetically set of labels \cite{bib4}. To do this, the ordered label sets contained within the ND-CG ratings of both the sibling subgroups are tuned simultaneously. A set of several generated trees is acquired via repetitive practice to enhance accuracy rate. So at moment of forecasting, every testing article for each inspired tree is relocated from the root node to a leaf node, as well as the label dividends across all of the attained leaves are added.


\subsection{Fast Text}
FastText is a simple and cost-effective deep learning-based solution to multi-class text categorization. A softmax function layer is used to transfer the signature analysis to class labels after they have been constructed by aggregating data embeddings of an article's keywords. This mapping step occurs once the textual portrayal is produced. Previous work on optimal text classification tasks, like skip-gram and CBOW, influenced the creation of this approach. \cite{bib8}. It builds interpretations of texts using a linear softmax classifier that overlooks the sequence of words in those articles. FastText is typically several orders of magnitude more efficient than competing approaches whilst giving state-of-the-art efficiency on a diverse collection of multi-class classification tasks. Although data briefings in XMLC require far additional data to accurately predict numerous linked labels and distinguish them from enormous amounts of meaningless labels, the achievement of a document-to-label modelling could've been restricted by simply aggregating its input embeddings and employing a simplistic architectural style. This occurs because the shallow design allows only a finite range of document-to-label mappings.

\subsection{CNN-KIM}
CNN-Kim was among the initial systems which use CNNs to classify text. CNN-Kim generates a textual vector by appending the word embeddings inside a data. After passing across t filters inside this convolution layer to produce t feature maps, the document vectors has been transferred to a softmax timed pooling layer to produce the t-dimensional model of data. These are continued by a layer which contains L softmax outcomes and compares those outcomes to L labels. \cite{bib9}. CNN-Kim has demonstrated remarkable efficiency for multi-class text categorization for implementations, making it an important baseline for the comparative study that we're conducting.

\subsection{BOW-CNN}
The Bow-CNN, commonly referred as the Bag-of-word CNN, is another excellent method for identifying a wide range of classifications. A one-hot vector, also referred as a bag-of-words indication vector, represents each unique small text area (several consecutive words). Every zones are issued the D-dimensional binary vector, with the $ith$ node receiving the value 1 if the lexical words related to that number could be discovered in the text of the given region. \cite{bib8}. The word D indicates the dimension of the feature region (the vocabulary). All of the region embeddings are first input into the convolutional layer, followed by a structure known as dynamic pooling, and ultimately in side of a layer known as softmax outcome. This process is continued until full article is displayed.

\subsection{PD-Parse}
The moniker PD-Sparse refers to a latest max-margin technique for categorising exceptionally huge quantities of labels. It has no position within those first three categories (target-embedding methods, tree-based methods, and deep learning methods). A linear classifiers having l1 and l2 penalty is generated in PD-Sparse and deployed to the weight matrix correlated with every label. Because it is very sparse within both the primary and dual areas, this leads in a technique that is advantageous again for performance of XMLC time as well as memory. Parses that are PD-Fully-Corrective Chunk The Frank Wolfe training strategy adopts use of the solution's sparseness to attain sub-linear time of training for the amount of both primary and secondary elements. on the other hand, Prediction time, remains to be linear. \cite{bib6}. Whenever it relates to multi-label classification, PD-Sparse outperforms 1-vs-all SVM and logistic regression while requiring significantly lesser time and resources for training the model.

\subsection{X-BERT}
The X-Bert approach was influenced by information retrieval (IR), that aims to find necessary documentation for a given query from a big pool of documents (BERT for eXtreme Multi-label Text Classification). When conducting searching and concurrently keeping a significant amount of documents, an IR system may frequently use the tactics described underneath.
\begin{itemize}
    \item Develop an efficient data structure to be used for indexing the content;
    \item By using matching technique, identify the content id that matches to this content example.
    \item Organize the documents in the produced list with in sequence you want them to appear.
\end{itemize}

The example that must be labelled may be matched to the query, and a huge proportion of labels may be matched to the large volume of documents stored by a search engine. In this sense, the XMC problem as well as the Information Retrival problem are linked. Due to the effectiveness of the Information Retrival's three-stage architecture with a large range of targets, certain modern approaches, such as HAXMLNet and Parabel, are somewhat comparable to this. X-BERT include a three-stage design, It includes the following stages as phases:

\begin{itemize}
    \item Making use of semantic label sorting,
    \item Deep learning is applied to connect label indexes.
    \item Bringing everything from the past phases' configurations together and arranging the labels based upon the indices obtained in each.
\end{itemize}

\section{Literature Discussion on XMLC Approaches}

In this section of the study, we examine previous studies that are applicable to our assessment. To start, i will examine studies that look into the variations and similarities in person's ability on titles and complete texts. After that, a brief overview of deep learning solutions for multi-label text classification is offered. Finally, we will examine many recent techniques to deep learning that are utilized to categorize text.

\subsection{Title versus Full-Text Approaches}
The research was carried out by \cite{bib7} is perhaps the most comparable to ours.   Using titles and complete texts from two different datasets, the researchers analyzed multi-label text categorization. Because they are likewise composed of scientific literature, two other datasets are identical to the ones utilized in this study. The authors tested both the full-text technique and the title-based approach with the same amount of samples. They discovered that title-based techniques might give good outcomes. In regards to the two specialized datasets, the gap among title and full-text stays at 10percentage  and 20 percentage, respectively, with full-text gaining the lead. The very first dataset is anchored in the economics research and is an updated version of the one used in this inquiry. The second set of data comprises information about political science. In this study, an MLP known as a Base-MLP outperforms every other classifier in terms of classification accuracy. This is the case in seven of the eight categorization matches. The bag-of-words (BoW) feature format is the backbone for each of the classifiers demonstrated here. This style has typically proved effective for categorising texts with regard for lexical items. Due to its undeniably increased performance, Base-MLP does have the ability to be recognised to be the most realistic depiction of ordinary BoW algorithms. As a result, we employ the efficiency of this model to evaluate the effectiveness of all subsequent models. We considered numerous other aims in addition to categorising the publications while comparing whole texts and info. The research examined the efficiency of leveraging [5] a title, summary, and content of a publication to create internet search enquiries. Following that, these inquiries are sent to numerous data sources available on the internet in order to acquire papers for suggestions. The abstract notions generated the most beneficial result in their studies, backed by physical existence. It is obvious that the title performs poorly. It has been demonstrated [6] that it is feasible to propose a competing article to a person depending solely on their Social Media profile, regardless if the indexes are created from article's title, abstract, or full-text. It was previously considered that this had been unachievable. This is made possible by their cutting-edge profile technique, HCF-IDF, that allocates engagement throughout a hierarchy skill set in order to obtain integrative theoretical data from the title[5]. Evaluates the efficacy of textual embedding techniques for the goal of information retrieval, regardless if the indexing is produced from the article's title, abstract, or full-text. Titles have shown to be more beneficial than abstracts and entire texts in this case. According to, full-text search, rather than meta-data search, provides the most efficient way to navigate the PubMed database[10].

\subsection{Multi-label Classification}
Text classification is a topic that has attracted a lot of attention. Support vector machines (SVM) and k-nearest neighbors (kNN) are two popular text categorization algorithms [8,14]. The complexities of k-nearest neighbors grows with the number of training samples, posing a difficulty for train large samples considered in this study. SVMs don't really perform exceptionally well when categorizing numerous labels. If there are multiple labels, however, using a binary relevance classification strategy becomes impractical. Specialists of the MeSH indexing group are presently investigating on multi-label text classification. The major focus of this topic is the tagging of PubMed papers with healthcare subject headings. The sixth and last phase of the BioASQ [13] challenge has been concluded. It provides huge training data in to accomplish its primary purpose, which is to improve the existing level of expertise in MeSH indexing. We understand the value of the MeSH index industry and based our research upon the most relevant BioASQ test datasets. However, the bulk of effective approaches to tackling this dilemma are biomedically relevant. Training to score [12,13,14] and patterns recognition [23] are two instances of effective solutions. These techniques are inapplicable since we are investigating domain-independent methods of categorizing topics in digital libraries in our project; therefore, these approaches are inapplicable.

\subsection{Deep Learning Classification}
At a small era, initial work included multi-label text classification as well as the usage of neural networks. [10] employ a bilateral sorting nonlinear function in their text classification research to fully capture labeled interrelations. As demonstrated in [14], cross-entropy, rather than ranking-loss, leads to faster convergence and overall greater projections of future. Furthermore, they are also the first to use major innovations that have occurred all through deep learning years. These innovations comprise rectified linear units, dropouts, and AdaGrad, an adaptive estimator. In current history, it has been demonstrated that neural networks outperform standard linear BoW algorithms inside the categorization of multi-class text, particularly on incredibly large data. Neural networks had demonstrated their abilities to function pretty well on a range of text classification databases with exceptionally small dimensions (up to 10.8k samples). Recurrent neural networks are one example.s \cite{bib9},convolutional neural networks (CNN) [15, 37], and blends of the 2 \cite{bib9}. By discovering the very first large and multi text categorization database,\cite{bib10}. The training data within those datasets ranged between 120k to 3.6 million. There are between two and fourteen classes to pick among. \cite{bib12} designed and compared a wide, character-based CNN to a range of traditional approaches, Traditional approaches included logistic regression multinomial using bag-of-ngrams and TF-IDF, together with models of deep learning like a Long Short-Term Memory network (LSTM) and CNNs. \cite{bib12} A word-based CNN was also proposed. The most important thing to note from our studies would be that conventional models usually outperform deep learning algorithm on the 4 comparatively small sets of data with 560k or lesser training sets; even so, the neural network methodology outperforms conventional approaches on the remaining 4 sets of data with 650k or even more training sets. Even though that all these statistics may fluctuate based on the data as well as the classification techniques that is presently being undertaken, this discovery is the fundamental reason for adopting deep learning algorithms in our inquiry. Recently, a growing number of deep learning tests have been performed on these data. \cite{bib13} were encouraged by the community of computer vision to boost the efficiency of word-based CNNs by extending their dimension. To bring these results into context, \cite{bib11}  consider that a CNN based on words performs exactly as well as, if not more than, the models utilized.As an outcome, we had limited our analysis to solely short CNNs which are charecter-based. When that refers to text classification, \cite{bib10}  it is debatable either CNNs or RNNs are preferable. As a consequence of this, As a result, hybrid approaches and LSTMs \cite{bib3, bib6} are usually effective if utilized with these massive datasets. As demonstrated, a linear MLP classifier might deliver comparable outcomes to non-linear deep learning algorithms while keeping the same degree of \cite{bib7}   computational efficiency. To my understanding, the current situation of art is provided by neural network varieties MLP, CNN, and LSTM on at minimum one of the extremely big datasets (as demonstrated in the research by \cite{bib15}]. I want to contribute to the discussion of neural networks while our study comprises simulations of every imaginable type of neural network. My research does have the ability to be classified as XMLC due to the large number of labels included in the data.

\section{Methods}
In this challenge, i demonstrate three distinct neural network architectures for text classification that we utilized throughout the study. The different types of neural networks are MLPs, CNNs, and Random Forest, and i focused on the entire pipeline trying to make sure that the models as precise as possible. \cite{johnson2014effective}. Just the complete structures applied in the procedures detailed in the subsequent section are displayed here. Such patterns may alter depending on the dataset being analysed in addition to the type of text being analyzed (titled or comprehensive). This is necessary to guarantee that databases and text structures are not biased. The output layer and training process are constant in each of the neural network architectures demonstrated. First, i will provide a summary of the training method before digging through every neural network architecture until the last hidden layer.

\subsection{Training}
TLexical labeling is indeed a multi-labeling challenge in which every article is assigned a series of tags instead of one single class. This discovery contradicts the vast bulk of past studies on text classification using large datasets. A popular way for modifying a classifiers to a scenario with many categories is to employ binary relevance. When there are significant amount of labels, this strategy could be costly because it necessitates training a same amount of classifications because there are labels. \cite{zhang2017} uses neural networks due to straightforward way they deal with the categorization of multiple labels. When performing multi-class text analysis, the output layer employs the activation function of softmax to provide a probabilistic model across all classes. Furthermore, exponential activation could be employed to assess the chance that every label must be assigned while doing multi-label categorization.

The Softmax method checks all categories at the same time, but the Sigmoid algorithm evaluates every label independently. If the plex exceeds a certain threshold, a binary method is used to decide whether or not to label the object. Adam is used to \cite{anderson83} train the networks to minimise the total of all binary cross-entropy failures throughout all categories. This aim is applicable to all labels. When it is used to multi-label text classification, this has been demonstrated to be significantly accurate than a loss based upon rank. \cite{nishioka2016profiling} \cite{blei2003latent}.

The training is performed in chunks of 256 components. Rapid termination is utilized not just as a normalization approach, but also as a criterion for training cessation. The sample-based F1-measure is employed on the verification set to conduct performance appraisal. Training is deemed finished whenever the validation score remains below or equal to the best possible result obtained for multiple consecutive evaluations \cite{collins2002discriminative}. Given that outcome of an activated sigmoid can be understood as possibility that perhaps a label should be given to an input, a criterion of 0.5 is an uncommon choice. This value may or may not yield the optimal results, according to the evaluation measure, database, or method used to compute assignment probabilities. Nevertheless, selecting an appropriate value might be a tremendously costly operation, especially for large datasets.

As a solution, we employ a heuristics that constantly alters the threshold value whereas the model is still being trained. To achieve this, optimisation is also performed utilizing the inspection of a testing set that was employed during the early stopping phase. We first intended to use a cutoff value of 0.2 because \cite{zhang20217} discovered that it can be far more precise than 0.5. We set I equals to argmax throughout this stage.
$$
k+i1,...,k+i1F1(Pi)
$$
where $k>0$ denotes the size of step. and k determines how many thresholds to verify after every test stage, whereas the classification algorithm predicts a chance for all of the |L|labels and every instance inside the testing dataset, resulting in:
$$
Pi(0,1)n|L|
$$
This strategy is based on the awareness that the greatest selection for is frequently quite close to the optimal solution for the previous evaluation stage, i1. We chose $k=3$ and $\theta =0.01$ to optimize efficiency while sacrificing accuracy owing to the probable cost of computing the $F1$ score. During our testing phase, this approach of threshold adjustment consistently generated excellent results, occasionally even outcome that have been superior to those achieved so when threshold was altered manually.

\subsection{Multi-layer Perceptron (MLP)}

The base model is the multi-layer perceptron (MLP) provided by \cite{zhang20217} Its intake is a TF-IDF \cite{zouaq2012voting} bundle of individual words, and it comprises 1 hidden layer with 1,000 units which have rectifying activation. The Bag of Individual words contains just 25,000 of the most common unigrams since its amount was judged to be sufficient for multi-labelallocation. The dropout\cite{li2016weighted} normalisation step follows the hidden layer with a preservation probability of 0.5.


This baseline should be known as Base-MLP. We enhance on the MLP built by \cite{zhang20217} by merging a few distinct techniques revealed in latest research on deep learning. This work presents the MLP structure, which is a multi-label classification variant of quick Text \cite{yao2014reader}. FastText is an upgraded linear BoW engine with a feature-sharing elementand local prepositional phrases information (bigrams). To follow this model, you must contain both the 25,000 most frequent unigrams and the 25,000 most frequent bi-grams.


Furthermore, we discovered that skipping this non-linearity just at hidden layer had a considerable impact on classifier performance. As an outcome, we'll use a nonlinear approach to trigger the hidden rectifier. As stated at the outset of this subsection, the greater the number of training sample, the higher equipped deep neural networks would be \cite{tuarob2013automatic}. This one is caused by the fact that as the amount of parameters increases, so does the expressive capability of neural networks. To accomplish this purpose, more layers could be added or existing levels might well be enhanced.


Deep neural networks, as demonstrated inside the vision domain \cite{collins2002discriminative} are also able to learn hierarchy input patterns. Deep neural networks, in contrast side, are infamous because they are more difficult to build than its shallow equivalents due to gradients disappearing and inflating issues. We employ the BatchNormalization \cite{zhang20217} approach to minimize the size of the deep MLPs. MLP differs from Base-MLP in that it combines bi-grams, has many layers, and performs average pooling only when needed.

\subsection{Convolutional Neural Network (CNN)}
We present a Deep convolutional neural network for text categorization, the main notion of which was introduced by Kim [15] and that has since been extensively adapted and built on wide range for investigators. A handful of these extra functionality are already available in the model. CNN's embeddings begin with a previously learned models, and they're improved more through training. Our CNN, like Kim's model, employs a 1D-convolution to extract the features at every place of a material by moving a pane over that. This is performed so that the content can be viewed. The results are then transformed using a non-linear activation function Following the detector stage,To identify the zone with maximum salient, max-pooling is frequently used. Instead, \cite{liu2012internet} split the convolutional output into nearly similar portions and afterwards performed max-pooling to every bits independently. The outcomes of the many operations that preceded it are then integrated. This structure is identical to Kim's for the value $p=1$. This process is frequently repeated for a range of dimensions. These findings are concated to be sent to the following layer. \cite{liu2012internet} has window sizes 2, 4, and 8, whereas Kim's CNN uses window sizes 3, 4, and 5. So according to our observations, those values 2, 3, 4, 5, and 8 produce significantly better results. With Kim's approach, the outcome of a pooling stages is instantly transmitted to the output layer. After that, the output is combined.
Meanwhile, \cite{liu2012internet} suggests that including an additional fully-connected layer comprising nodes, which they term to be the layer of cnn, is helpful because it offers the network a greater model precision. This layer is known as the set of layers. The difficulty of expanding the capabilities of CNNs is exacerbated by the diversity of our databases and the amount of training instances. Wider convolutions  or more layers of overlaid convolution layers, comparable to MLPs, might be employed to increase capacity. We solely look at the first technique since more latest evidence by \cite{lewis04} has shown that depth seems to have no advantage over shallow nets.

\section{Setup}

\subsection{Data}
To collect datasets in English, we employed two separate library resources of research papers. EconBiz is a search engine that specializes on economic and business studies. It presently has 2,485,000 English articles, 615k among which are accessible to everyone. By using "Thesaurus for Economics" (STW) set of topic headings, industry professionals has marked a significant number of publications. We collected the headlines and remarks from all 2,485,000 English articles. After redundancies are deleted, there seem to be currently 1,064,634 articles with notes left. The volume of articles with tags and full texts that may be retrieved and examined falls to 70,619, accounting for 6.63 percent of all open source articles.

PubMed is a browser for publications in biomedicine and the biosciences that is run by National Library of Medicine in the USA. Humans have contributed "Healthcare Category Titles" tags to the papers listed in PubMed (MeSH). We were able to acquire a dataset by utilizing the training examples for 2017 BioASQ contest's semantics index assignments \cite{johnson2014effective}. This database is produced entirely in English and comprises millions of article headlines as well as MeSH tags.

The USA National Library of Medicine operates a database called PubMed Central6 that comprises full-text articles in the biomedical science and biomedicine. The most of its 4.3 million freely accessible publications are written in English. But, only 1.5 million are fully accessible, implying that text analytics not an option for them \cite{bhowmick2009multi}. We used a formula to calculate the overlap of this database with the articles collected for BioASQ challenge. After redundancies are removed, there are a maximum of 12,834,026 titles and 646,513 full phrases with related comments.

As a consequence, 5.04 percent of the examples include the entire text. Many features of the datasets EconBiz and PubMed are shown in the table below for your review. Since there are additional categories among which to select the paper's tags, the work of tagging the PubMed database is much more challenging because of the database's computational complexities. Despite this, both sets of information may be viewed as XMLC problems7 due to the typical low quantity of samples linked with every label combined with the relatively large number of tags \cite{schulz2015rapid} \cite{DBLP:journals/pr/MadjarovKGD12}. The headlines in PubMed are significantly lengthier on average than those in EconBiz.


The relevance of this conclusion became evident whenever one realizes that PubMed title frequently have more categories to forecast than words. So when full texts of the articles are reviewed, both databases do have equal amount of words compared to labels. It is also important to remember that titles datasets frequently have one fewer category than full-text databases. This divergence is significant. This implies that the labeling proportions in the full-text dataset vary dramatically from the ones in the title database \cite{Wang:2012:BBS:2390665.2390688}. It is critical to remember that the set of title for both databases is a specialized version of a full-text set.

\subsection{Experiments}
We build sub-datasets of the title datasets by continuously adding additional data in order to examine the behavior of title-based algorithms as a greater number of titles are considered for training. Our goal is to build these sub-datasets so that we can examine the behavior of title-based algorithms as a greater number of titles are Figure 1 demonstrates this point well. It is necessary to test the trained model with the same data in order to conduct an impartial comparison of the performance of titles and complete texts. In order to do this, we perform a cross-validation that is divided into ten folds and divide the collection of publications that include entire texts into ten parts. At the beginning of each cycle, nine folds are chosen for the sake of training, and one is picked for testing. The validation set, which will be used for early stopping and threshold adjustment as described in Section 3.1, is comprised of twenty percent of this training set and is chosen at random for inclusion. Because they provided the data that we needed for our full-text testing, we will refer to them as EconBiz Full and PubMed Full, respectively.

The same articles that were reviewed during the 10-fold cross-validation are now being evaluated for the title experiments. The titles dataset is continually added to the training set in order to ensure that the total number of training samples is always a power of two of the number of training samples in the full-text experiment. This is done in order to meet the requirements of the power of two guarantee. We test on five sub-datasets of titles for each domain, which are designated as $T1, T2, T4, T8,  T_{all}$. $Tx$ denotes that there are x times as many title samples used for training purposes in the dataset as there are full-text examples. The dataset's title samples are all included in the $Tall$ file. The four different classifiers Base-MLP, MLP, CNN, and LSTM are all put through their paces on each individual sub-dataset. In all, we do 48 different types of cross-validation. This statistic was selected to describe the data because, according to \cite{goossen2011news} it best portrays how topic indexers work.

In addition to that, we also utilize this statistic for early pausing and adjusting the threshold while working with the validation set. The sample-based $F1$ measure begins by computing the harmonic means of recall and precision for each individual sample, and then it takes these values and takes an average of them over all of the samples.

\subsection{Parameter Selection Choice}
Selecting the Appropriate Hyperparameters and Undertaking Training Because deep learning contains a huge number of hyperparameters that can be defined, tweaking a large number of hyperparameters simultaneously may be a time-consuming and labor-intensive process. This is particularly true when the datasets being used are enormous. It would not be acceptable for our research to standardize the hyperparameter settings across all datasets and models due to the fact that the datasets and model architectures are quite distinct from one another and may need relatively varied values for the hyperparameters. We came to the conclusion that the best solution would be to compromise by adjusting the hyperparameters for full-texts and titles separately on a single fold, although in very minute increments. In this section, we changed each hyperparameter in a decentralized fashion and chose the full-text and title solutions that worked the best in the local environment. It is critical to keep in mind that the settings for titles for $T_{all}$  served as the basis for determining the settings for all other title sub-datasets. On the one hand, this results in a considerable reduction in the amount of money spent on processing, and it also makes it possible to assess how well different title sub-datasets perform. On the other hand, there is a possibility that poor performance will result from overfitting, particularly for smaller sub-datasets.

During the analysis, this fact has to be taken into account. In every one of our MLP studies, we use a one-layer MLP with 2,000 units and a dropout probability of 0.5 after the hidden layer. For the trials on the PubMed titles, we use a two-layer MLP with 1,000 units in each layer and no dropout at any point. Batch Normalization is what is used instead, and it is done after each buried layer. Adam's starting learning rate is consistently set at 0.001, regardless of the circumstances. In each of our full-text CNN experiments, the bottleneck layer is given the parameters p=3chunks and $nb=1,000$ units [19]. Regarding the titles, we do not chunk the data $(p = 1)$ and make use of a bottleneck layer size of $nb = 500$. The size of the feature map is 400 pixels in all of the tests with the exception of PubMed Full, where it is just 100 pixels. The initial learning rate is 0.001, and the probability is kept at its default value of 0.75 at all times. All tests utilise a single-layer LSTM.

We performed tests on both sets of data and found that the optimal number of memory cells for titles is 1,536. While EconBiz Full utilizes just 512 units, the PubMed Full database utilizes 1,024 units. The keep probability is set to 0.75 throughout all trials, with the exception of the PubMed on titles experiment, where it is set to 0.5. 0.01 is the starting learning rate for EconBiz Full, whereas 0.001 is the starting rate for all other instances \cite{scikit-learn}. Back propagation, which involves unrolling the LSTM until the sequence reaches its conclusion, is how training is carried out.

The preprocessing and tokenization method developed by \cite{gibaja2014multi} is the one that we use. We employ 300-dimensional pre-trained word embeddings for the LSTM and the CNN. These embeddings were created via 840 billion tokens of \cite{song2014short} training on Common Crawl. We get rid of any words that aren't in our lexicon. The first 250 words of a sequence can never exceed the maximum length allowed for that sequence. In the first stages of testing, lengthier sequences were shown to be dangerous. TensorFlow9, a library for deep learning, was used in the construction of our neural network models, \cite{katz53} and \cite{yao2014reader}, a framework for multi-label classification, was utilized to include these models. All experiments are done on Essex HPC.

\section{Reflection}
Thee experiment results are shown in the tables below:

\begin{table}[htb]
 \centering \caption{Full Text Experiment $F_1 Scores$ Results.}
\begin{tabular}{ccc}
    \hline
    \textbf{MODEL} & \textbf{ECONBIZ} & \textbf{PUBMED} \\
    \hline
    \hline
    Base MLP & 0.191& 0.194 \\
     \hline
    MLP & 0.226& 0.212 \\
     \hline
    CNN & 0.181& 0.193\\
     \hline
     Random Forest & 0.317& 0.291\\
     \hline
     \end{tabular}
    \end{table}

Referring to the data, it is obvious that a reasonable multi-label classification of textual information could be accomplished using only the article titles. Throughout all databases, the Random Forest implemented to titles preserves 31 percentage of the F-score acquired from full-text analysis. Here is an actual argument for the benefits of automatic semantic article tagging utilizing metadata. Based on the outcomes of the initial experiment, using a mixture of words with thoughts developed from them is more successful than using merely one of those independent. Ideas disclose details that is significant and conceptually related to a particular region. The recurrence of the terms, in contrast side, provides extremely useful and critical explicit data for correct classification.
Figure 1 illustrate the outputs of the baseline MLP, MLP, CNN, LSTM,Random Forest.

\eightpt
\bibliographystyle{IEEEtran}
\bibliography{task3bib.bib}
	
\end{document}